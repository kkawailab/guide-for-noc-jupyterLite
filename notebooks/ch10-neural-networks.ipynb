{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915dbaac",
   "metadata": {},
   "source": [
    "# 第10章 — ニューラルネットワーク\n",
    "\n",
    "- パーセプトロンは重み付き和に活性化をかけ、層を重ねて関数を近似する。\n",
    "- 学習では損失の勾配降下で重みを更新する。\n",
    "- 非線形活性化（ReLUやシグモイド）が曲線的な決定境界を表現する。\n",
    "- データの正規化と小さめの学習率が安定性を高める。\n",
    "\n",
    "試してみよう: 活性化をReLUに変えたり、OR/XORを学習して（XORは中間層が必要）挙動を試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mathdef sigmoid(x):    return 1 / (1 + math.exp(-x))class Perceptron:    def __init__(self, w, b=0.0, lr=0.1):        self.w = w        self.b = b        self.lr = lr    def predict(self, x1, x2):        z = self.w[0] * x1 + self.w[1] * x2 + self.b        return sigmoid(z)    def train(self, data, epochs=20):        for _ in range(epochs):            for x1, x2, label in data:                pred = self.predict(x1, x2)                error = label - pred                self.w[0] += self.lr * error * x1                self.w[1] += self.lr * error * x2                self.b += self.lr * error# AND gate datasetsamples = [    (0, 0, 0),    (0, 1, 0),    (1, 0, 0),    (1, 1, 1),]p = Perceptron([0.1, 0.1], b=-0.1, lr=0.5)p.train(samples, epochs=30)for x1, x2, _ in samples:    print((x1, x2), \"->\", round(p.predict(x1, x2), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

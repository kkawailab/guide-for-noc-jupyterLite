{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915dbaac",
   "metadata": {},
   "source": [
    "# 第10章 — ニューラルネットワーク\n",
    "\n",
    "- パーセプトロンは重み付き和に活性化をかけ、層を重ねて関数を近似する。\n",
    "- 学習では損失の勾配降下で重みを更新する。\n",
    "- 非線形活性化（ReLUやシグモイド）が曲線的な決定境界を表現する。\n",
    "- データの正規化と小さめの学習率が安定性を高める。\n",
    "\n",
    "試してみよう: 活性化をReLUに変えたり、OR/XORを学習して（XORは中間層が必要）挙動を試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mathdef sigmoid(x):    return 1 / (1 + math.exp(-x))class Perceptron:    def __init__(self, w, b=0.0, lr=0.1):        self.w = w        self.b = b        self.lr = lr    def predict(self, x1, x2):        z = self.w[0] * x1 + self.w[1] * x2 + self.b        return sigmoid(z)    def train(self, data, epochs=20):        for _ in range(epochs):            for x1, x2, label in data:                pred = self.predict(x1, x2)                error = label - pred                self.w[0] += self.lr * error * x1                self.w[1] += self.lr * error * x2                self.b += self.lr * error# AND gate datasetsamples = [    (0, 0, 0),    (0, 1, 0),    (1, 0, 0),    (1, 1, 1),]p = Perceptron([0.1, 0.1], b=-0.1, lr=0.5)p.train(samples, epochs=30)for x1, x2, _ in samples:    print((x1, x2), \"->\", round(p.predict(x1, x2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e5f8d",
   "metadata": {},
   "source": [
    "### 追加例: 2層パーセプトロンでXOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0863bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        random.seed(0)\n",
    "        self.w1 = [[random.uniform(-1,1) for _ in range(2)] for _ in range(2)]\n",
    "        self.b1 = [0.0, 0.0]\n",
    "        self.w2 = [random.uniform(-1,1) for _ in range(2)]\n",
    "        self.b2 = 0.0\n",
    "        self.lr = 0.5\n",
    "    def forward(self, x1, x2):\n",
    "        h1 = sigmoid(self.w1[0][0]*x1 + self.w1[0][1]*x2 + self.b1[0])\n",
    "        h2 = sigmoid(self.w1[1][0]*x1 + self.w1[1][1]*x2 + self.b1[1])\n",
    "        out = sigmoid(self.w2[0]*h1 + self.w2[1]*h2 + self.b2)\n",
    "        return (h1, h2), out\n",
    "    def train(self, data, epochs=5000):\n",
    "        for _ in range(epochs):\n",
    "            for x1, x2, label in data:\n",
    "                (h1, h2), out = self.forward(x1, x2)\n",
    "                error = label - out\n",
    "                # 出力層勾配\n",
    "                d_out = error * dsigmoid(out)\n",
    "                # 隠れ層勾配\n",
    "                d_h1 = d_out * self.w2[0] * dsigmoid(h1)\n",
    "                d_h2 = d_out * self.w2[1] * dsigmoid(h2)\n",
    "                # 重み更新\n",
    "                self.w2[0] += self.lr * d_out * h1\n",
    "                self.w2[1] += self.lr * d_out * h2\n",
    "                self.b2 += self.lr * d_out\n",
    "                self.w1[0][0] += self.lr * d_h1 * x1\n",
    "                self.w1[0][1] += self.lr * d_h1 * x2\n",
    "                self.w1[1][0] += self.lr * d_h2 * x1\n",
    "                self.w1[1][1] += self.lr * d_h2 * x2\n",
    "                self.b1[0] += self.lr * d_h1\n",
    "                self.b1[1] += self.lr * d_h2\n",
    "\n",
    "mlp = MLP()\n",
    "data = [(0,0,0),(0,1,1),(1,0,1),(1,1,0)]\n",
    "mlp.train(data, epochs=3000)\n",
    "for x1, x2, label in data:\n",
    "    (_, _), out = mlp.forward(x1, x2)\n",
    "    print((x1, x2), \"->\", round(out, 3), \"label\", label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
